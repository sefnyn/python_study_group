# -*- coding: utf-8 -*-
"""NLP Accessions data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KrwgJ942N1Q2XHrhrSIa8FQqW135NqR9

# **NLP: Analysing text**

An attempt to use natural language processing to analyse accessions data
"""
# This was originally a .ipynb hence no print statements, but was too large to push to GitHub

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
import pandas as pd

df_acc = pd.read_excel('accessions_data_2023.xlsx')
df_acc.head()

#Brief outlline of the data:
df_acc.info()
df_acc.dtypes

#extracting the description column of the dataset
all_text = df_acc['Description'].to_string()

#text = df['Description'].str.replace(r'[^\w\s]+', '').to_string()

df_acc.info()

#1. Segmentation:
#Breaking the text down to sentences
from nltk.tokenize import sent_tokenize
sentences = sent_tokenize(all_text)
#print(sentences[0])
len(sentences)

#2. Tokenization:
#Extracting words as tokens
from nltk.tokenize import word_tokenize
all_tokens = nltk.word_tokenize(all_text)
print(len(all_tokens))

#Extracting the word tokens of the first sentence

sentence1 = sentences[0]
tokens_sentence1 = word_tokenize(sentence1)
print(tokens_sentence1)
print(len(tokens_sentence1))

#3. Parts of speech (POS):
#Identifying the parts of speech of each word
nltk.pos_tag(tokens_sentence1)

# Using SpaCy to extract and explain POS
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp(sentence1)
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.is_stop)

import pandas as pd
cols = ("text", "POS", "explain", "stopword")
rows = []
for t in doc:
    row = [t.text, t.pos_, spacy.explain(t.pos_), t.is_stop]
    rows.append(row)
df = pd.DataFrame(rows, columns=cols)

df

df['explain'].value_counts()
# transforming this to a dataframe allows a closer look at the make up of the text
# much of this sample of descriptions are made up of proper nouns and nouns, with verbs acounting for relatively little of the text

#4. Stemming and Lemmatization:
from nltk.stem import PorterStemmer
ps = PorterStemmer()
for w in word_tokenize(sentences[0]):
    rootWord=ps.stem(w)
    print(rootWord)

# Stemming provides the root stem of a word, which can help im retrieving related terms or information
# however, the morphological variants Porter Stemmer produces are not always real words, and it can return meanings or spellings that can look like they relate words other than the original root word
# lemmatization can therefore be more accurate, as it converts the word to it's base form. But lemmatisation takes longer
ps = PorterStemmer()
word_freq = dict()

for w in word_tokenize(sentence1):
    rootWord=ps.stem(w)

    # Create an empty dictionary

    if rootWord not in word_freq:
            word_freq[rootWord] = 1
    else:
            word_freq[rootWord] += 1
print(word_freq)

# Lemmatization: aims to resolve words to their dictionary form; stems returned through lemmatization are semantically complete
# This can be useful in analysing word usage in a corpus, and considering how this has changed over time
import nltk
nltk.download('wordnet')
nltk.pos_tag(tokens_sentence1)
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

for token in word_tokenize(sentence1):
  print(lemmatizer.lemmatize(token,  pos ="v"))

#Lematization with SpaCy
#SpaCy has a lemma_ atribute, which makes lemmatization very easy, though NLTK is more customisable
doc = nlp(sentence1)
for token in doc:
    print(token.text + '  ===>', token.lemma_)

# Stopword identification:
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from nltk.tokenize import word_tokenize

filtered_text = []

for w in all_tokens:
    if w not in stop_words:
        filtered_text.append(w)
print(filtered_text)

# Removing stopwords allows us to see the most common relevant words in the text,
# these can then be plotted on a graph to measure frequency

from nltk import FreqDist
df_f = pd.DataFrame(filtered_text, columns =['Desc'])

text1 = df_f['Desc'].str.replace(r'[^\w\s]+', '').str.lower().to_string()#to remove punctuation for the graph
#print(text1)
tokens1 = nltk.word_tokenize(text1)
fdist = FreqDist(tokens1)
fdist.plot(20)

# The graph shows that there are many common words used in archival descriptions, that, while important, are too prevalent to allow a closer view of what the records are really about
# so we can create a new stopword list (with words such as papers and records) and add this to the NLTK stopwords
# this will allow a clearer view of the most common themes

stop_words = set(stopwords.words('english'))

fileWords = open("extra stopwords.txt")
line = fileWords.read()
filestop = line.split()
#print(filestop)

filestop.extend(stop_words)
print(filestop)

filtered_text1 = []
for w in tokens1:
    if w not in filestop:
        filtered_text1.append(w)
print(filtered_text1)
fdist = FreqDist(filtered_text1)
fdist.plot(30)

# The concordance allows us to see the context of words in the text
# for example, 'digital' seems to largely refer to copies or images
text2 = nltk.Text(all_tokens)
text2.concordance('digital', width=150, lines=60)

text2.concordance('court', width=100, lines=50)
#'court' refers mostly to court registers, but also includes manorial documents (court book, court Leet, court roll)

import matplotlib.pyplot as plt
text2.dispersion_plot(['history', 'digital', 'war'])
#a dispersion plot can show where words are located in the text. This isn't so useful for accessions data, but does help show density, and this would be interesting for analysing historical texts

#finding words with a similar range of context:
print(text2.similar('church'))
print('--------')
print(text2.similar('war'))

# extracting words that often go together within the text
text2.collocations()

# This helps us to see additional context. For example, we can see that war often refers specifically to the world wars, and marriage relates to specific documents - marriage registers

#6. Dependency Parsing: extracting relationships
doc = nlp(sentence1)
for token in doc:
    print(token.text, ':',  token.pos_, ':', token.dep_)

# extracting noun chunks ...
for chunk in doc.noun_chunks:
    print(chunk.text)

#... and showing their dependencies
for token in doc:
    print(token.text, ':', token.dep_, ':', token.head.text, ':', token.head.pos_,
            [child for child in token.children])

from spacy.symbols import nsubj, VERB
verbs = set()
for possible_subject in doc:
    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:
        verbs.add(possible_subject.head)
print(verbs)

import pandas as pd
cols = ("text", "POS", "explain POS", "dependency", "explain dep", "stopword")
rows = []
for token in doc:
    row = [token.text, token.pos_, spacy.explain(token.pos_), token.dep_, spacy.explain(token.dep_), token.is_stop]
    rows.append(row)
d_df = pd.DataFrame(rows, columns=cols)
d_df.head(30)
# SpaCy is fairly good at extracting POS and the related dependency, though does tend to over-predict proper nouns - possibly because of the extensive use of capitalisation (and the use of capitalised case) in the text

# Visualising dependancy parsing
from spacy import displacy

doc = nlp(sentence1)
displacy.render(doc, style="dep", jupyter=True)

#7. Name Entity Recognition:
# Extracting and classifying specific entities (persons, places, organisations etc.) within the text
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")

doc = nlp(sentence1)
colors = {"ORG": "linear-gradient(90deg, #aa9cfc, #fc9ce7)"}
options = {"ents": ["NORP", "GPE", "LOC", "EVENTS", "PERSON", "ORG"], "colors": colors} #limiting the ents to those most relevant to the text
displacy.render(doc, style="ent", jupyter=True, options=options)

# SpaCy can produce a nice visualisation, colour coding the named entities. While this looks nice, it is less useful for a very long text
# but there are definite limitations with NER with this type of text

# counting the values classified under each entity and extracting the most common values
from collections import Counter
doc = nlp(all_text)
labels = [ent.ent_type_ for item in doc.ents for ent in item]
print(Counter(labels))

items = [(ent.ent_type_, ent.text) for item in doc.ents for ent in item]
Counter(items).most_common(5)

# It's not all that precise, classifying photographs as organisations (just as it sometimes assigned them proper nouns)

# transforming the entity information to a dataframe to explore it farther
import pandas as pd

doc = nlp(all_text)

cols = ("Text", "Entity Type", "Explain Ent Type", "IOB code")
rows = []

entities = [(ent.text, ent.ent_type_, spacy.explain(ent.ent_type_), ent.ent_iob_)for item in doc.ents for ent in item]
#https://stackoverflow.com/questions/53755893/facing-attributeerror-for-tag-using-spacy-in-python

df_ent = pd.DataFrame(entities, columns=cols, index=None)
df_ent.head(40)

df_ent["Entity Type"].value_counts()

df_ent.groupby("Entity Type").count().filter(["Text"])  #or as a dataframe

df_norp = df_ent.loc[df_ent["Entity Type"]== "NORP"].filter(["Text"]).value_counts()
df_norp.head(20)

# The default model in SpaCy doesn't quite work for accessions data.
# The data contains a lot of lists of words and large mix of entities, some of which change depending on context.
# The ideal would be to train a new model using previous accessions data.
# It does pick up quite a few correct entities, but also includes a few oddities (like Photographs as NORP - Nationalities or religious or political groups), and also classifies minutes as time rather than in the context of notes from meetings

df_gpe = df_ent.loc[df_ent["Entity Type"]== "GPE"].filter(["Text"]).value_counts()
df_gpe.head(15)
#There are a lot of place-names in the dataset, many of which aren't recognised

df_event = df_ent.loc[df_ent["Entity Type"]== "EVENT"].filter(["Text"]).value_counts()
df_event.head(15)

from nltk import FreqDist

df_org = df_ent.loc[df_ent['Entity Type']=='ORG'].filter(['Text'])
df_org.head()

text_ent = df_org['Text'].str.replace(r'[^\w\s]+', '').str.lower().to_string()#to remove punctuation for the graph

tokens_ent = nltk.word_tokenize(text_ent)
fdist = FreqDist(tokens_ent)
fdist.plot(20)

# visualising organisations to show the most common. Although SpaCy classes recordings and cuttings as organisations, we can clearly see that councils, churches and schools are predominant.

#Spacy has an entity-ruler  that allows the addition of 'spans' or patterns to enhance the named entity recogniser, which can help improve accuracy
#It's possible to implement a rule-based entity recognition system with this method

import spacy
from spacy.lang.en import English
from spacy.tokens import span
from spacy.pipeline import EntityRuler

nlp = spacy.load("en_core_web_sm")
ruler = EntityRuler(nlp, overwrite_ents=True)
patterns = [{"label": "GPE", "pattern": "Australia"},
            {"label": "GPE", "pattern": "India"},
            {"label": "GPE", "pattern": "China"},
            {"label": "GPE", "pattern": "Asia"},
            {"label": "EVENT", "pattern": [{"LOWER": "expedition"}]},
            {"label": "EVENT", "pattern": [{"LOWER": "carnival"}]},
            {"label": "EVENT", "pattern": [{"LOWER": "protest"}]}]
ruler.add_patterns(patterns)
t_text = "Albums and loose photographs relating to the service of an unidentified soldier in China during the 1920s. Soldier possibly belonged to the Suffolk Regiment. Includes images of soldiers and civilians in China; celebration of Armistice Day in 1927; barricades and a protest; Japanese soldiers; Cantonese officers and men; American Marines and machine guns; a Jiu-Jitsu tournament; Indian police officers; British soldiers playing mah-jong; British soldiers relaxing off-duty; the aircraft Spirit of Canton flown by Chang Wai-jung; HMS SUFFOLK in China; visits to the zoo; Chinese funeral processions; Chinese Flight Officers; Shanghai; Nanking; Wei-Hai-Wei. Chinese street scenes; postcard views of Shanghai; Sikh police officer directing traffic. Loose formal postcard prints of unidentified British soldiers, possibly of the Suffolk Regiment. includes a notebook with comments on various places in Asia, Australia and Africa and a diary account of the return sea voyage from China to England."
#text taken from an accession description as a quick test of the model.
doc1 = nlp(t_text)
for ent in doc1.ents:
  if ent.label_ == "GPE":
    print((ent.text, ent.label_))

# Spacy also allows phrase matching with similar results
import spacy
from spacy.lang.en import English
from spacy.matcher import PhraseMatcher
from spacy.tokens import span
matcher = PhraseMatcher(nlp.vocab)
nlp = English()

new_events = [nlp.make_doc(text) for text in ('Expedition', 'Protest', 'Carnival')]
countries = [nlp.make_doc(text) for text in ('Australia', 'India', 'China', 'Asia')]

matcher = PhraseMatcher(nlp.vocab, attr="LOWER")

matcher.add('EVENTS', None, *new_events)
matcher.add('GPE', None, *countries)

doc = nlp(all_text)

matches = matcher(doc)
for match_id, start, end in matches:
    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID
    span = doc[start : end]
    print(rule_id, span.text, (match_id, start, end))

#https://stackoverflow.com/questions/47638877/using-phrasematcher-in-spacy-to-find-multiple-match-types



from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt

wordcloud = WordCloud(stopwords=filestop, background_color='black', colormap='magma_r', width=1600, height=1200, max_words=200).generate(all_text)

plt.figure(figsize=(10,8), frameon=True)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

# visualising the most common words in the accessions dataset